---
title: "ML Workshop2 Week 5"
author: "John Navarro"
date: "July 21, 2017"
output: pdf_document
---

# 1 Regressions Trees

```{r}
suppressWarnings(library(rpart))
suppressWarnings(library(rpart.plot))
suppressWarnings(library(ISLR))
suppressWarnings(library(knitr))
suppressWarnings(library(caret))
suppressWarnings(library(MASS))
```
## 1.1 Predicting Baseball Players' Salaries

Use the Hitters data from the ISLR library, sec 8.1.1 in ISLR
```{r}
data("Hitters")

Hitters <- na.omit(Hitters)
kable(head(Hitters,3))
```

Predict a baseball players Salary based on the number of Years spent in the majors and the number of Hits the previousyear. Salary is measured in thousands of dollars
Fitthe model
```{r}
salaryFit <- rpart(Salary~Hits+Years, data=Hitters)
prp(salaryFit,extra=101, # display the number of observations that fall in the node
    branch=.5, # change angle of branch lines
    shadow.col="gray", # shadows under the leaves
    branch.lty=3, # draw branches using dotted lines
    split.cex=1.2, # make the split text larger than the node text
    split.prefix="is ", # put "is " before split text
    split.suffix="?", # put "?" after split text
    split.box.col="lightgray", # lightgray split boxes (default is white)
    split.border.col="darkgray", # darkgray border on split boxes
    split.round=.5,
    nn=TRUE) # display the node numbers, default is FALSE
```
It appears that the tree needs to be pruned. It is only considering hits and years, and gets too complicated on the right.
Lets quantitatively analyze how to prune the tree based on parameter CP and the error columns.
```{r}
salaryFit$cptable
# compare x error to rel error plus xstd
cbind("RE+STD"= salaryFit$cptable[,3]+salaryFit$cptable[,5], "xerror"=salaryFit$cptable[,4])
```
Rule of thumb shows that the tree should be pruned at 3 splits or at CP 0.020522
```{r}
# prune tree after 3rd split
prunedTree <- prune(salaryFit, cp=salaryFit$cptable[3,1])
printcp(prunedTree)

#Display pruned tree
prp(prunedTree,extra=101, # display the number of observations that fall in the node
    branch=.5, # change angle of branch lines
    shadow.col="gray", # shadows under the leaves
    branch.lty=3, # draw branches using dotted lines
    split.cex=1.2, # make the split text larger than the node text
    split.prefix="is ", # put "is " before split text
    split.suffix="?", # put "?" after split text
    split.box.col="lightgray", # lightgray split boxes (default is white)
    split.border.col="darkgray", # darkgray border on split boxes
    split.round=.5,
    nn=TRUE) # display the node numbers, default is FALSE
```

This tree is smaller, it separates the players by experience, then splits the experienced group by hits.
Try pruning by finding the best CP level just by xerror
```{r}
printcp(salaryFit)
```
```{r}
plotcp(salaryFit)
(best.CP <- salaryFit$cptable[which.min(salaryFit$cptable[,"xerror"]),"CP"])
prunedTree <- prune(salaryFit, cp = best.CP)
printcp(prunedTree)
prp(prunedTree,extra=101, # display the number of observations that fall in the node
    branch=.5, # change angle of branch lines
    shadow.col="gray", # shadows under the leaves
    branch.lty=3, # draw branches using dotted lines
    split.cex=1.2, # make the split text larger than the node text
    split.prefix="is ", # put "is " before split text
    split.suffix="?", # put "?" after split text
    split.box.col="lightgray", # lightgray split boxes (default is white)
    split.border.col="darkgray", # darkgray border on split boxes
    split.round=.5,
    nn=TRUE) # display the node numbers, default is FALSE
```
After the tree is pruned forecasting is done by averaging the output based on mean values of the tree leafs.
Finally, it is  possible to look at the residuals from this model, just as with a regular linear regression fit.

```{r}
plot(predict(prunedTree),resid(prunedTree))
temp <- prunedTree$frame[prunedTree$frame$var == '<leaf>',]
axis(3, at = temp$yval, as.character(row.names(temp)))
mtext('leaf number', side = 3, line = 1)
abline(h=0, lty=2)

```
We see that  node 6 has many players andgood concentration around the mean. Node 2 has some outliers.
Find the MSE of prediction
```{r}
(prunedTree.MSE <- sum((Hitters$Salary - predict(prunedTree))^2))
# 2nd MSE calculation method
sum(residuals(prunedTree)^2)

```
Use library(caret) to build the same tree
```{r}
set.seed(0)
# 10 fold cross validation
ctrl <- trainControl(method="cv", number=10)
tree.slice <- train(Salary~Hits+Years, data = Hitters, method="rpart", trControl=ctrl)
tree.slice$results

```
Plot the tree
```{r}
prp(tree.slice$finalModel,extra=101, # display the number of observations that fall in the node
    branch=.5, # change angle of branch lines
    shadow.col="gray", # shadows under the leaves
    branch.lty=3, # draw branches using dotted lines
    split.cex=1.2, # make the split text larger than the node text
    split.prefix="is ", # put "is " before split text
    split.suffix="?", # put "?" after split text
    split.box.col="lightgray", # lightgray split boxes (default is white)
    split.border.col="darkgray", # darkgray border on split boxes
    split.round=.5,
    nn=TRUE) # display the node numbers, default is FALSE
```
Calculate MSE
```{r}
# MSE of caret tree method
caretTree.MSE <- sum((residuals(tree.slice$finalModel))^2)

# Compare the two MSE values
c(rpart=prunedTree.MSE,caret.rpart=caretTree.MSE)

#Plot the residuals from caret tree method
matplot(#1:length(resid(prunedTree)),
        cbind(resid(prunedTree),residuals(tree.slice$finalModel)), pch=c(16,1),col=c("black","red"), ylab="Residuals", xlab="Index")
```
Here we can see that both models give the same residual values for each Hitter.

## 1.2 Examples from the introduction to the package Rpart

## 1.3 Introduction to main functions: Cars

The dataset car90 contains a collection of variables from the April, 1990 Consumer Reports; it has 34 variables on 111 cars.Variables tire size and model name are excluded because they are factors with a very large number of levels which creates a very long printout, and rim size because it is too good a predictor of price and leads to a less interesting illustration. (Tiny cars are cheaper and have small rims.)
```{r}
# remove rows, use -match with column names to select columns in car90 subset
cars <- car90[, -match(c("Rim", "Tires", "Model2"), names(car90))]
# return head of cars
head(cars)

# create regression tree
carfit <- rpart(Price/1000~., data=cars)
carfit

# plot carfit tree
prp(carfit,extra=101, # display the number of observations that fall in the node
    branch=.5, # change angle of branch lines
    shadow.col="gray", # shadows under the leaves
    branch.lty=3, # draw branches using dotted lines
    split.cex=1.2, # make the split text larger than the node text
    split.prefix="is ", # put "is " before split text
    split.suffix="?", # put "?" after split text
    split.box.col="lightgray", # lightgray split boxes (default is white)
    split.border.col="darkgray", # darkgray border on split boxes
    split.round=.5,
    nn=TRUE) # display the node numbers, default is FALSE

# Print the complexity parameter table for carfit
printcp(carfit)
```
Re run with the cp threshold at .001
```{r}
summary(carfit, cp=0.1)
```
plot the observed-expected cost of cars vs predicted cost of cars
```{r}
plot(predict(carfit), jitter(resid(carfit)))
# extract the rows that are leafs
temp <- carfit$frame[carfit$frame$var == '<leaf>',]
# label the leaf numbers
axis(3, at = temp$yval, as.character(row.names(temp)))
# add the label leaf # to top margin
mtext('leaf number', side = 3, line = 1.5)
# draw the zero line
abline(h = 0, lty = 2)
```
Examine how Rsqd improves and how the X relative error drops with growing number of splits
```{r}
par(mfrow=c(1,2))
rsq.rpart(carfit)
```

## 1.4 Poisson regression: sod example

The Poisson splitting method attempts to extend rpart models to event rate data.
The model in this case is ??=f(x), where ?? is an event rate and x is some set of predictors. The solder data frame, is a dataset with 900 observations which are the results of an experiment varying 5 factors relevant to the wave-soldering procedure for mounting components on printed circuit boards.

The response variable, skips, is a count of how many solder skips appeared to a visual inspection.

The other variables are:
Opening factor: amount of clearance around the mounting pad (S<M<L)(
Solder factor: amount of solder used (Thin<Thick)
Mask factor: type of solder mask used (5 possible)
PadType factor: Mounting pad used (10 possible)
Panel factor: panel (1, 2 or 3) on board being counted

In this call, the rpart.control options are modified:

maxcompete = 2 means that only 2 other competing splits are listed (default is 4);
cp = .05 means that a smaller tree will be built initially (default is .01).
The y variable for Poisson partitioning may be a two column matrix containing the observation time in column 1 and the number of events in column 2, or it may be a vector of event counts alone

```{r}
head(solder,20)
sfit <- rpart(skips ~ Opening + Solder + Mask + PadType + Panel,data = solder, method ='poisson',
              control = rpart.control(cp = 0.004, maxcompete = 2))
printcp(sfit)
#summary(sfit)
```
```{r}
# Plot the tree
prp(sfit,extra=101, # display the number of observations that fall in the node
    branch=.5, # change angle of branch lines
    shadow.col="gray", # shadows under the leaves
    branch.lty=3, # draw branches using dotted lines
    split.cex=1.2, # make the split text larger than the node text
    split.prefix="is ", # put "is " before split text
    split.suffix="?", # put "?" after split text
    split.box.col="lightgray", # lightgray split boxes (default is white)
    split.border.col="darkgray", # darkgray border on split boxes
    split.round=.5,
    nn=TRUE) # display the node numbers, default is FALSE
```
The response value is the expected event rate(with a time variable), or in this case the expected number of skips
The deviance is the same as the null deviance(sometimes called the residual deviance) that you'd get when calculating a Poisson glm model for the given subset of data
```{r}
summary(sfit, cp=0.1)
```

Note the splitting criterion. The improvemnt is Dev(parent) - (DevR + DevL)
Which is the likelihood ratio test for comparing two Poisson samples
```{r}
fit.prune <- prune(sfit, cp=0.1)
printcp(fit.prune)
```
Plot the pruned tree
```{r}
prp(fit.prune,extra=101, # display the number of observations that fall in the node
    branch=.5, # change angle of branch lines
    shadow.col="gray", # shadows under the leaves
    branch.lty=3, # draw branches using dotted lines
    split.cex=1.2, # make the split text larger than the node text
    split.prefix="is ", # put "is " before split text
    split.suffix="?", # put "?" after split text
    split.box.col="lightgray", # lightgray split boxes (default is white)
    split.border.col="darkgray", # darkgray border on split boxes
    split.round=.5,
    nn=T) # display the node numbers, default is FALSE
```
The function prune() trims the tree fit to the cp =0.10
the same tree could have been created by specifying cp = 0.10 in the original call to rpart()

Prepare function for calculating Residual Mean square error of the model
```{r}
rmse <- function(x) sqrt(mean(x^2))
#calc the measure of fit of the pruned tree
treeRmse<-rmse(resid(fit.prune))
```
Calculate probability of zero count by the Poisson tree model for row 6 of data frame solder
```{r}
(preTree <- predict(fit.prune, newdata=solder[6,]))
# probability of zero count
(pred0Tree<-dpois(0,lambda=preTree))
```

## 1.4.1 Exploring switches on a tree

In this section explore application of rules of regression tree to each row of observations.
```{r}
library(plyr)
library(dplyr)
library(data.tree)
library(partykit)
```
prepare a function creating a matrix of all paths from root to leaf on a given tree
```{r}
rpartPaths<-function(x){
  require(plyr)
  require(partykit)
  require(rpart)
  require(data.tree)
  dtTree<-as.Node(as.party(x)) # transform object into data.tree
  myPaths<-lapply(dtTree$leaves,     # find all paths from root to leave on the tree
                  function(z) t(as.matrix(as.numeric(FindNode(dtTree,z$name)$path))))
  myPaths<-rbind.fill.matrix(myPaths)  # create matrix with variable row lengths
  myPaths
}

# Analyze unpruned tree which has the following configuration
plot(as.Node(as.party(sfit)))
```
Plotting a different way
```{r}
plot(as.Node(as.party(sfit)),output = 'visNetwork')
```
Create the matrix of paths
```{r}
# Use function created above to determine all the paths and which nodes they go through
(pts <- rpartPaths(sfit))

# create a vector of all the termina nodes
# omitting NAs for each row, select the last value in each
(lvs <- apply(pts, 1, function(z) tail(na.omit(z),1)))

# Create table of numbers of observations in terminal nodes
# $where gives terminal node for each observation. 
# table is tallying how many observations end at each terminal node
(distrInLeaves <- table(sfit$where))

# Create vector of predictions by unpruned tree
Pred<-round(predict(sfit,newdata=solder),2)

# Collect in one matrix: the data, predictions, terminal nodes and entire paths for each observation

completeTreeMatrix <- cbind(solder, Pred, Where=sfit$where, pts[match(sfit$where, lvs),])
head(completeTreeMatrix)
```
Explore switches at different levels of the tree.
```{r}
completeTreeMatrix[58:62,]
```
At 61,Opening switched from M to S and padType switched from L9 to W4, Both changes resulted in a jump of predicted value from 0.17 to 1.16, the change of the paht on the tree was from node 2 to node 21 at the second level
```{r}
completeTreeMatrix[88:92,]
```
Here we see at 91, the path switched back from node 21 to node 2, at level 2 as a result of change in opening and pad type
```{r}
completeTreeMatrix[448:462,]
```
At 451 there is a big change at level 2 as a result of a change in opening, solder and pad type.
Between 451 and 462 switches at level5asa result of panel changes. Panel 1 is different from 2 and 3
```{r}
completeTreeMatrix[506:512,]
```
At 511 a big change as a result of switchin opening
```{r}
completeTreeMatrix[538:548,]

```
From 538 to 548 jittering in level 5 due to pad type switches.

It is possible to select pruning level based on intensity of switches.
```{r}
apply(completeTreeMatrix[,10:15],2,
      function(z) sum(diff(na.omit(z))!=0)/length(na.omit(z)))

# What is !=0 doing here

```
Noise jumps at level 5. Plot switches at each level
```{r}
plot(completeTreeMatrix[,10], type="l", main="Level 2", ylab="Node #")
```
```{r}
plot(completeTreeMatrix[,11], type="l", main="Level3", ylab= "Node#")
```
```{r}
plot(completeTreeMatrix[,12], type="l", main="Level 4", ylab="Node #")
```
```{r}
plot(completeTreeMatrix[,13], main="Level 5", type="l", ylab="Node #")
```
```{r}
plot(completeTreeMatrix[,14], type="l", main="Level 6", ylab="Node#")
```
```{r}
plot(completeTreeMatrix[,15], type="l", main="Level 7", ylab="Node #")
```
Note that at level 6, there is a big switch at 511, from node 11 to 36, resulting in change in predicted counts from 4 to 16. But this level is not necessary because closer to the root at level 2 there is a swtich from node 2 to 21

Prune tree to 4 levels. This correpnds to cp level of 
```{r}
printcp(sfit)
prunedFit4 <- prune(sfit, cp=0.0339887)
prp(prunedFit4,extra=101, # display the number of observations that fall in the node
    branch=.5, # change angle of branch lines
    shadow.col="gray", # shadows under the leaves
    branch.lty=3, # draw branches using dotted lines
    split.cex=1.2, # make the split text larger than the node text
    split.prefix="is ", # put "is " before split text
    split.suffix="?", # put "?" after split text
    split.box.col="lightgray", # lightgray split boxes (default is white)
    split.border.col="darkgray", # darkgray border on split boxes
    split.round=.5,
    nn=T) # display the node numbers, default is FALSE
```

```{r}
# Plot just the node numbers
plot(as.Node(as.party(prunedFit4)))
```
```{r}
plot(as.Node(as.party(prunedFit4)),output = 'visNetwork')
```

## 1.4.2 Negative Binomial Regresssion Model

Poisson regression does not fit these data well. There is significant overdispersion. Instead, we used negative binomial regression.
Fit negative binomial regression and compare residuals of both models

```{r}
modnb <- glm.nb(skips ~ ., solder)
summary(modnb)
```

Plot the residuals of both the poisson tree and the Negative binomial
```{r}
matplot(1:length(modnb$residuals),cbind(modnb$residuals,resid(sfit)),type="p",pch=c(1,19),xlab="Index",ylab="Residuals",main="Comparison of Tree Regression and NB Regression")
legend("bottomleft",legend=c("NB","Tree"),col=c("black","red"),lty=1,lwd=3)
```
Calculate residual mean square error of the negative binomial model
```{r}
(nbRmse <- rmse(modnb$residuals))

# Calculate the probability of zero count by the negative binomial model for row 6 of data frame solder
mu <- predict(modnb, solder[6,], type="response")
# density for neg bin distribution. 
pred0NB <- dnbinom(0, mu=mu, size=modnb$theta)

# Compare mrse of the 2 models
c(Tree_RMSE=treeRmse, NB_RMSE=nbRmse)

# Compare the probabilities of zero count for row 6 of the data given by the 2 models
c(tree_0Probability=pred0Tree,NB_0Probability=pred0NB)
```

# 2. Time series of stock prices

Predict returns of exchange traded fund SPY representing S&P 500 with a group of stock returns of companies in the index. Select year 2014

```{r}
datapath <- "C:/Users/JohntheGreat/Documents/MSCA/MachineLearning/Week5_Trees/"
SPYPortf<-read.csv(paste(datapath,"spyPortfolio.csv",sep="/"))
head(SPYPortf,3)
```
Create daily log returns of all stocks and SPY
Make daily log returns of all stock prices lagged one day relative to the daily log returns of SPY
```{r}
# take the log of all the data
SPYPortf<-log(SPYPortf)

#check the data
head(SPYPortf)

# Lag the data one day relative to the daily log returns
SPYPortf <- apply(SPYPortf,2,diff)

#check the data
head(SPYPortf)

```
Calculate meta features as PCA factors
```{r}
# Run PCA on the log differenced data (not including the SPY)
SPYPortfPCA <- princomp(SPYPortf[,-28])

# extract the factor scores of the first the principal components
SPYPoptf.factors<-SPYPortfPCA$scores[,1:3]

# create a data frame with 3 factor scores and the log diff of SPY
SPYPortfFactors<-as.data.frame(cbind(SPYPoptf.factors,SPY=SPYPortf[,28]))

# return the head of the new data frame
head(SPYPortfFactors)
```
Grow a regression tree
```{r}
# create a regression tree, using anova
tsfit <- rpart(SPY ~.,data = SPYPortfFactors, method ='anova',
              control = rpart.control(cp = 0.001))
# print cp table
printcp(tsfit)

# plot the regression tree
prp(tsfit,extra=101, # display the number of observations that fall in the node
    branch=.5, # change angle of branch lines
    shadow.col="gray", # shadows under the leaves
    branch.lty=3, # draw branches using dotted lines
    split.cex=1.2, # make the split text larger than the node text
    split.prefix="is ", # put "is " before split text
    split.suffix="?", # put "?" after split text
    split.box.col="lightgray", # lightgray split boxes (default is white)
    split.border.col="darkgray", # darkgray border on split boxes
    split.round=.5,
    nn=TRUE) # display the node numbers, default is FALSE
```
Prune the tree
```{r}
(best.CP <- tsfit$cptable[which.min(tsfit$cptable[,"xerror"]), "CP"])
```
Since the suggested level is 7 nodes deep, let's prune a littel less than suggested by best.CP
```{r}
# Prune tsfit at node 11
prunedTsFit <- prune(tsfit, cp= 0.0022440)

# Plot the pruned tree
prp(prunedTsFit,extra=101, # display the number of observations that fall in the node
    branch=.5, # change angle of branch lines
    shadow.col="gray", # shadows under the leaves
    branch.lty=3, # draw branches using dotted lines
    split.cex=1.2, # make the split text larger than the node text
    split.prefix="is ", # put "is " before split text
    split.suffix="?", # put "?" after split text
    split.box.col="lightgray", # lightgray split boxes (default is white)
    split.border.col="darkgray", # darkgray border on split boxes
    split.round=.5,
    nn=TRUE) # display the node numbers, default is FALSE
```
Interpret the tree:
Principal component 1 is inversely related to returns of the SPY. As Comp 1 gets larger, we see that the return is more negative, similarly, as Comp 1 gets more negative, we see that the return is more positive. This Component describes the common relation ship of member stocks to the overall broad market index(represented by the SPY)
Component 2 seems to be considered on the right side of the tree (where returns are positive). But here we see inconclusive splits.

For stocks with component 1 between 0.0095 and -0.0083 we see that a component 2 score GREATER than -0.013 leads to a smaller return, almost zero. While a component score less than -0.013 gives us a slightly positive return. 
Component 2 also comes into play when Comp 1 is between -0.03 and -0.069, but here we see that if Comp 2 is LESS than -0.015 we get a smaller return and if Comp2 is GREATER than -0.015 we get a positive return.

Create a matrix of paths
```{r}
# Use function created above to identify paths
(tsSPY<-rpartPaths(prunedTsFit))

# Plot the nodes
plot(as.Node(as.party(prunedTsFit)))
```
Create vector of all terminal nodes
```{r}
# use tail() with na.omit to find the terminal node values
(lvsSPY <- apply(tsSPY, 1, function(z) tail(na.omit(z),1)))

# Create a table of the numbers of observations in terminal nodes
table(prunedTsFit$where)

# Create vector of predictions by the pruned tree
PredSPY <- round(predict(prunedTsFit, newdata=SPYPortfFactors),6)

# Combine in one matrix, the data, the predictions, terminal nodes and the entire paths
completeMatrixSPY <- cbind(round(SPYPortfFactors, 4), PredSPY, Where=prunedTsFit$where, tsSPY[match(prunedTsFit$where, lvsSPY),])

# return the head of complete matrix
head(completeMatrixSPY)
```
In general the tree regression model predicted actual returns reasonably well
```{r}
# Comparing predicted returns vs actual returns
plot(completeMatrixSPY[,"PredSPY"],type="l",col="blue",lwd=3,
     ylim=range(completeMatrixSPY[,"SPY"]))
lines(completeMatrixSPY[,"SPY"],col="orange")
legend("top",legend=c("Predicted","Actual"),col=c("blue","orange"),lwd=c(2,1))

# Scatter plot of predicted vs acutal
plot(completeMatrixSPY[,"PredSPY"],completeMatrixSPY[,"SPY"])
```

# 2.1 Example. Large number of Predictors

Consider the example analysed in the workshop Linear Regression with Large Number of Predictors
Simulate data
```{r}
# 500 values
N=500

# set seed for reproducibility
set.seed(0)

# Create error terms that are random normal distribution
Epsilon <- rnorm(N,0,1)

# Create X terms random normal distribution
X <- rnorm(N*N, 0,2)

# Alter X terms into a matrix
dim(X) <- c(N,N)

#Set col names as Xn
colnames(X) <- paste0("X", 1:N)

# Calculate slopes as random uniform distribution 500 from 1 to 3
slopesSet <- runif(N, 1,3)

# Using a linear combination of the values created above
Y <- sapply(2:N, function(z) 1+X[,1:z]%*%slopesSet[1:z]+Epsilon)

# display head of matrix X
head(X[,1:5])
```
Fit linear regression model with 440 regressors
```{r}
m=440
completeModelDataFrame <- data.frame(Y=Y[,m-1],X[,1:m])
dim(completeModelDataFrame) #500x441
m440 <- lm(Y~., data=completeModelDataFrame)
#summary(m440)
```

We reduced the number of regressors in completeModelDataFrame in order to ensure that on each iteration of 10-fold cross validation, the number of regressors is less than the number of observations. (The matrix is longer than it is wide)
The residual standard error of the model is
```{r}
summary(m440)$sigma

```

Apply regresssion tree to these data
```{r}
treeFit <- rpart(Y~., data=completeModelDataFrame)
printcp(treeFit)
plotcp(treeFit)
```
Here we see that the trees can't handle this data at all. Cross validatioi error increases almost monotonically. It means that any split implies OVERFITTING and the best tree based prediction model is constnat
RSE of the constant prediction is
```{r}
sd(completeModelDataFrame$Y)
```
Use caret to run cross validation
```{r}
ctrl <- trainControl(method="cv", number = 10)
lmTrain <- train(Y~., data=completeModelDataFrame, method="lm", trControl=ctrl)
lmTrain$results
```

