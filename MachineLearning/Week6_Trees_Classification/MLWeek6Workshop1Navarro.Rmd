---
title: "MLWeek6Workshop1"
author: "John Navarro"
date: "July 28, 2017"
output: pdf_document
---
# Introduction
## 1.1 Classification

In machine learning and statistics, classification is the problem of assigning a new observation to one of several predefined categories. Solution is obtained on the basis of a training set of data containing observations (or instances) whose category membership is known (supervised learning).

Denote 
X is the set of objects
Y is a finite set of classes
function: X->Y
Lm = (x1,f(x1)),....(xm,f(xm)) is the learning sample

Function f classifies objects correctly, but we are given only set Lm. Hence we know function f only on the subset of X. The goal is to find an algorithm classifying all objects from X

Generalization of an "ordinary" classification is probabilistic classification that is able to predict, given a new object, a probability distribution over a set of classes, rather than only outputting the most likely class taht the objectshould belong to.

##1.2 Classification performance evaluation:2 classes

Begin with assessing binary classification quality.
Suppose that only 2 object types are possible
Y={y1,Y2}
A confusion matrix is a table that is often used to describe the performance of a classification model(or "classifier") on a set of test data for which the true values are known. 

### Example

A total of 1000 people were tested for a disease.
Out of those 1000 people, the test returned:
Positive results(predicting disease) 180 times
Negative results(predicting no disease) 820 times

Further test showed that in reality 900 of tested people did not have disease(Y2) and 100 people had it (Y1). For the 80 patients with disease test results came back positive and for other 20 diseased patients the test results were negative. Then the corresponding confusion matrix is

```{r}
confusion=data.frame('Tested negative'=c(800,20), 'Tested Positive'=c(100,80))
rownames(confusion)=c("No disease", "disease")
colnames(confusion)=c("Tested negative", "Tested positive")
confusion
```

Introduce some related notations

True Positive TP: These are cases in which people who test positive had the disease
True Negative TN: These are cases in which people who test negative did not have the disease
False Positive FP: Test cam back positive, but the person had no disease (Type 2 error)
False Negative FN: Test came back negative, but the person had the disease (Type 1 error)

Confusion matrix is a 2x2 matrix containing values TP, Tn, FP, FN. 

The statistics commonly calculated from confusion matrix are:
```{r}
TN=confusion[1,1]
FP=confusion[1,2]
FN=confusion[2,1]
TP=confusion[2,2]
total=sum(confusion)
Disease=sum(confusion[2,])
Nodisease=sum(confusion[1,])
TestedPositive=sum(confusion[,2])
TestedNegative=sum(confusion[,1])

# Accuracy: overall how often is the classifier correct?
accuracy=(TP+TN)/total

# Misclassification rate: Overall how often is it wrong. Equivalent to 1 minus accuracy, also known as "Error Rate"
misclassificationRate=(FP+FN)/total

# True Positive Rate: When the person has disease, how often test result is positive? It is also known as Sesitivity or Recall
truePositiveRate=TP/Disease

#False Positive Rate: Given that the person has no disease, how often the test result is positive
falsePositiveRate=FP/Nodisease

# Specificity: Given that the person has no disease, how often is the test negative? Equivalent to 1 minus False Positive Rate
specificity=TN/Nodisease

# Precision: When test is positive, how often is it correct?
precision=ifelse(TP==0,0,TP/TestedPositive)

# Prevalence: What is the proportion of people with the disease
prevalence=Disease/total
```

### 1.2.1 Accuracy

Accuracy defined above is often the starting poing for analyzing the quality of a predictive model, as well as an obvious criterion for prediction.
It is the first characteristic the package caret shows in cross validation results.
Accuracy measures the ratio of correct predictions to the total number of cases evaluated.
It may seem obvious that the ratio of correct predictions to cases should be a key metric. 
But be careful! a predictive model may have high accuracy, but be useless.

Accuracy paradox: it is possible to use a degenearte classification algorithm that simply assigns "negative" class to all visits. This can have a higher accuracy than a model that separates.

###1.2.2 Cohen'sKappa

The second characteristic that package caret shows in cross validation results is Cohen's Kappa. This is essentially a measure of how well the classifier performed as compared to how well it would have performed simply by chance. 
The expression for kappa is k=(Accuracy-RandomAccuracy)/(1-RandomAccuracy)

Similar to correlation coefficient, Kappa can range from -1 to +1, where 0 represents the quality that can be expected from random classifier. and 1 represents perfect predicting quality. While kappa values below 0 are theoretically possible, Cohen notes that they are unlikely in practice. As with all correlation statistics, the kappa is a standardized and unitless vlaue and thus has universal interpretation. Calculate Cohen's Kappa for the accuracy paradox confusion matrix assigning every observation to "Negative"




```{r}
# Set matrix values
TN=10150
FN=150
FP=0
TP=0
TestedPositive=250
# Calcualte Random Accuracy
RA=(((TN+FP)*(TN+FN))+((FN+TP)*(TP+FP)))/(TN+FN+FP+TP)
RA
# Calculate Accuracy
ACC=(TP+TN)/(TN+FN+FP+TP)
ACC
# Calculate Cohen's Kappa
k = (ACC-RA)/(1-RA)
k


```

# 1.2.3 F-measure
```{r}
# Fraud is Positive
TN=10000
FN=50
FP=150
TP=100
TestedPositive=250
# Disease is sum of 2nd row
Disease=150

# Sensitivitiy or Recall
(Recall <- TP/Disease) #0.6666667

# Precision
(precision = ifelse(TP==0,0,TP/TestedPositive))#0.4

F=(2*precision*Recall)/(precision+Recall)
F #0.5

# Fraud is Negative
TN=100
FN=150
FP=50
TP=10000
TestedPositive=10050
Disease=10150
# Sensitivitiy or Recall
(Recall <- TP/Disease) #0.9852217

# Precision
(precision = ifelse(TP==0,0,TP/TestedPositive)) #0.9950249

F=(2*precision*Recall)/(precision+Recall)
F #0.990099

```

### 1.2.4 ROC curve

ROC curve is commonly used graph that summarizes the performance of a classifier over all possible thresholds. It is generated by plotting the True Positive Rate(y-axis) against the False positive rate(x-axis) for different values of threshold for assigning observations to a given class
AUC (Area Under Curve) is a good way to summarize ROC performance in a single number
The origin of term "ROC" is signal preocessing. It is an acronym for Receiver Operating Characteristics.
ROC curve was first developed by electrical engineers and radar engineers during WW2 for detecting enemy objects in battlefields and was soon introduced to psychology to account for perceptual detection of stimuli

In the next example consider ROC curve for logistic regression classififcation of two 2 dimentional normal samples

Install packages
```{r}
library(ggplot2)
library(Deducer)
```
 
```{r}
# normal distributions parameters
meanP=c(0,0)
sdP=c(2,1)
meanN=c(1,1)
sdN=c(1,2)

# sample sizes
nP=100
nN=300

#simulate 2 samples
dataROC <- data.frame(x = c(rnorm(nP,meanP[1],sdP[1]),
                            rnorm(nN,meanN[1],sdN[1])),
                       y = c(rnorm(nP,meanP[2],sdP[2]),
                             rnorm(nN,meanN[2],sdN[2])),
                       class = c('Positive','Negative')[1+c(rep(1,nP),rep(0,nN))]) 
ggplot(dataROC, 
       aes(x=x, y=y, color=class)) + 
  geom_point()+ 
  scale_color_hue(l=65, c=100)+ 
  scale_color_manual(values=c("orange", "blue"))
```

Getting ROC curve for logistic regression
```{r}
model.glm <- glm(formula=class~., family=binomial("logit"), data=dataROC)
rocplot(model.glm)
```

# 2 Recursive Partitioning Using the RPArt package

## 2.1 Example Manual implementation of Gini impurity measure

To understand Gini impurity measure create a simple vector containing 2 classes:
Create predictor and data frame.
```{r}
library(rpart)
library(rpart.plot)
```
```{r}
ClassesTest <- c(0,1,1,0,1)
PredictorsTest <- c(1,2,3,4,5)
TestData <- as.data.frame(cbind(PredictorsTest, ClassesTest))
plot(TestData)
```
Run classification tree algorithm
```{r}
testTree <- rpart(ClassesTest~PredictorsTest, data=TestData, control=rpart.control(cp=0.00, minbucket = 1, minsplit = 2, method="class"))
printcp(testTree)
```
The table shows the splits, the cost function for the complexity of the tree(cp) and the two errors:before and after cross validation
Visualize the tree
```{r}
prp(testTree,extra=4, # display prob of classes in the node
    branch=.5, # change angle of branch lines
    shadow.col="gray", # shadows under the leaves
    branch.lty=3, # draw branches using dotted lines
    split.cex=1.2, # make the split text larger than the node text
    split.prefix="is ", # put "is " before split text
    split.suffix="?", # put "?" after split text
    split.box.col="lightgray", # lightgray split boxes (default is white)
    split.border.col="darkgray", # darkgray border on split boxes
    split.round=.5)
```



## 2.2 Example: Mixture model

Create a mixture model of 2 gaussian distributions
```{r}
library(nor1mix)
```
```{r}
set.seed(3890)
# Assign two sets of means/sd/weights, plot the mixed sample
norMixObj <- norMix(mu=c(3,4.5), sigma = c(.4,.5), w=c(.6,.4))
plot(norMixObj)

```
```{r}
mixModelSample<-rbind(cbind(rnorm(600,mean=3,sd=.4),rep(0,600)),cbind(rnorm(400,mean=4.5,sd=.5),rep(1,400)))
hist(mixModelSample[,1])
plot(density(mixModelSample[,1]))
```
Reshuffle the rows
```{r}
mixModelSample <- mixModelSample[sample(nrow(mixModelSample)),]
library(mclust)
EstimatedModel <- Mclust(mixModelSample[,1], modelNames="V")
names(EstimatedModel)
EstimatedModel$parameters
#sd
sqrt(EstimatedModel$parameters$variance$sigmasq)

```

Plot theoretical density, density estimated without mix model and density estimated by the model
```{r}
mclust1Dplot(mixModelSample[,1], parameters = EstimatedModel$parameters,what = "density", xlab = "PPM",lwd=2,ylim=c(0,.7))
lines(density(mixModelSample[,1]),col="red",lwd=2)
lines(norMixObj,col="blue",lwd=2)
legend("topright",legend=c("Theo","No Model","Mclust"), lty=1,lwd=2,col=c("blue","orange","black"))
```
Show the classification uncertainty
```{r}
rule<-mixModelSample[which.max(EstimatedModel$uncertainty),1]
mclust1Dplot(mixModelSample[,1], parameters = EstimatedModel$parameters,what = "uncertainty",z=EstimatedModel$z, xlab = "PPM")
abline(v=rule)
```
where vertical line is at 3.7039228.

This is the classification rule given by mclust.
The classification plot shows that.
```{r}
mclust1Dplot(mixModelSample[,1], parameters = EstimatedModel$parameters,what = "classification",z=EstimatedModel$z, xlab = "PPM")
abline(v=rule)
```

Plot classification errors
```{r}
mclust1Dplot(mixModelSample[,1], parameters = EstimatedModel$parameters,what = "errors",truth=mixModelSample[,2] ,z=EstimatedModel$z, xlab = "PPM")
```
Now do the same classification using tree
Note that mclust did the clustering. This method does not use training sample like rpart does. Of course, in exchange mclust knowas about the model assumption: normal mixed model

prepare the data for rpart and grow the tree
```{r}
colnames(mixModelSample)<-c("X","Y")
head(mixModelSample)
mixModelSample <- as.data.frame(mixModelSample)
myMixTest <- rpart(Y~X, data = mixModelSample, control=rpart.control(cp=0.00, minbucket = 1, minsplit=2), method="class")
printcp(myMixTest)
prp(myMixTest,extra=4, # display prob of classes in the node
    branch=.5, # change angle of branch lines
    shadow.col="gray", # shadows under the leaves
    branch.lty=3, # draw branches using dotted lines
    split.cex=1.2, # make the split text larger than the node text
    split.prefix="is ", # put "is " before split text
    split.suffix="?", # put "?" after split text
    split.box.col="lightgray", # lightgray split boxes (default is white)
    split.border.col="darkgray", # darkgray border on split boxes
    split.round=.5)
```
This tree is large, but xerror shows that it can be pruned
```{r}
# returns the CP value of the row with the lowest cross validation erro
(best.CP <- myMixTest$cptable[which.min(myMixTest$cptable[,"xerror"]),"CP"])
```
```{r}
# Prune the tree
myMixTest.pruned <- prune(myMixTest, best.CP)

# plot the pruned tree
prp(myMixTest.pruned,extra=4, # display prob of classes in the node
    branch=.5, # change angle of branch lines
    shadow.col="gray", # shadows under the leaves
    branch.lty=3, # draw branches using dotted lines
    split.cex=1.2, # make the split text larger than the node text
    split.prefix="is ", # put "is " before split text
    split.suffix="?", # put "?" after split text
    split.box.col="lightgray", # lightgray split boxes (default is white)
    split.border.col="darkgray", # darkgray border on split boxes
    split.round=.5)
```
```{r}
summary(myMixTest.pruned)
```
Compare the classification rule with that of mclust
```{r}
c(ruleMclust = mixModelSample[which.max(EstimatedModel$uncertainty),1], ruleTree=myMixTest.pruned$splits[4])

```

## 2.3 Example: Otto Product Classification

In this example we will use data set, provided by Otto Group for Kaggle competition Otto Group Product Classification Challenge.

The goal of the project was categorization about 200,000 unspecified products into 9 classes based on 93 unspecified features.
For simplicity we reduced in this example the number of products and the number of classes.

### 2.3.1 Structure of the Data

Read the data
```{r}
library(fBasics)
library(knitr)
```

```{r}
datapath <- "C:/Users/JohntheGreat/Documents/MSCA/MachineLearning/Week6_Trees_Classification"

Data = read.csv(paste(datapath,'DTTrain.csv',sep='/'),header=TRUE)
```
Explore the training data
```{r}
dim(Data)
kable(head(Data[,1:5],3))
kable(Data[1:3,(ncol(Data)-4):ncol(Data)])
```
Each column represents an integer feature
Each row corresponds to a product
The first column is the time of information acquisition. The last column is the target variable, ie the product class. This column is excluded from the test data
```{r}
# last column name, which is the class, response
classColName=tail(colnames(Data),1)
Data[1:5, classColName]
```
The class variable is a character string in the 95th column and is called target

The rows of the training data set are sorted in ascending order of class numbers
In order to apply cross validation, a built in part of rpart, we need to reshuffle the rows before the analysis.
Set seed for reproducibility.
```{r}
set.seed(8)
xTrain=Data[sample(nrow(Data)),-1] # remove time stamp
dim(xTrain)
```

### 2.3.2 Creating and Plotting Small Classification Trees

Create formula for the tree.
```{r}
(formula=formula(paste(classColName,'.',sep='~')))

```

build ther ree. use one control parameter min bucket
```{r}
set.seed(8)
smallTree <- rpart(formula, data=xTrain,control = rpart.control(minbucket=250,cp=.0001))
printcp(smallTree)
```
The table in the output shows the summary of the fitted tree.
Rows correspond to different sizes of the tree
The table has the following columns
CP- shows complexity parameter controlling the tree size
nsplit- number of splitting nodes in the tree
rel error-the error of the training set relative to the error with no splits
xerror -  the relative error of the test set calculated with kfold cross validation
xstd standard deviation of xerror

Plot cross validation x error vs tree size and complexity parameter
```{r}
plotcp(smallTree)
```
the horizontal dotted line shows the level of 1 standard error above the minimum of xerror

## 3.2 Example. Rectangular class domain

Each observation is three-dimensional vector with two numeric features and one categorical (type) with possible values 'Positive' or 'Negative'.
The goal is to identify type on the basis of first two features.

```{r}
N = 1000
xPos = 0.2
yMinPos = 0.2
yMaxPos = 0.7
newData = data.frame(x=runif(N),y=runif(N))
newData$type = with(newData,ifelse(x>xPos & y>yMinPos & y<yMaxPos,
                        'Positive','Negative'))
```
Add noise and reshuffle
```{r}
n = N/10
newData$type[1:n] = c('Positive','Negative')[1+rbinom(n, 1, 0.5)]
newData = newData[sample(nrow(newData)),] 
```
Plot the data
```{r}
ggplot(newData, aes(x=x, y=y, color=type)) + geom_point()+ scale_color_hue(l=65, c=100)+ scale_color_manual(values=c("orange", "blue"))
head(newData)
max(newData$y)
```
Fit logistic regression and tree model with caret and compater their predictive quality

```{r}
# use caret() package to create both models

# Set the model formula
newModelFormula=formula('type~.')

# use train() for glm
logrTrain <- train(newModelFormula, data=newData,method="glm", trControl=ctrl)

# use train() for rpart
treeTrain <- train(newModelFormula, data=newData, method="rpart", trControl=ctrl)

# return logistic regression results
logrTrain$results

# return tree results
treeTrain$results
```
Here we can see that the Kappa from Logistic regression (0.14)is significantly lower than the Kappa from Tree model(0.879/0.765)



#4 Classification

## 4.1 otto Product

```{r}
suppressWarnings(library(fBasics))
Data = read.csv(paste(datapath,'DTTrain.csv',sep='/'),header=TRUE)
dim(Data) # dataset dimensions
```
 
### 4.1.1 Random forest
 
 eta learning rate, if its small, converges longer, but more accurate
```{r}
rfOtto <- randomForest(target~., data=xTrain,ntree=100)
rfPred <- predict(rfOtto, xTest, type="prob") # predict probabilities
rfPred[1:5,]
```

#### 4.1.2 Gradient boosting

```{r}
set.seed(13)
testInd = sample(nrow(Data), nrow(Data)/3)
xTrain = Data[-testInd,-1]
xTest = Data[testInd,-1]
yTrain = as.factor(Data$target[-testInd])
yTest = Data$target[testInd]
xgbTrain = data.matrix(xTrain[,-ncol(xTrain)])
xgbTest = data.matrix(xTest[,-ncol(xTest)])
yTrain = as.integer(yTrain)-1
yTest = as.integer(yTest)-1
table(yTrain)
```
```{r}

numClasses = max(yTrain) + 1
param <- list("objective" = "multi:softprob",
              "eval_metric" = "mlogloss",
              "num_class" = numClasses)
```

```{r}
cv.nround <- 10
cv.nfold <- 3
set.seed(1)
(bst.cv = xgb.cv(param=param, data = xgbTrain, label = yTrain, 
                nfold = cv.nfold, nrounds = cv.nround,verbose=F))
```

```{r}
rf_target_IndMat<-dummy.data.frame(data=as.data.frame(yTest), 
                                     sep="_", verbose=F, 
                                     dummy.class="ALL")
rf_target_IndMat[1:10,]
(rfLogLoss = MultiLogLoss(rf_target_IndMat,rfPred))
bst = xgboost(param=param, data = xgbTrain, label = yTrain, 
              nrounds=cv.nround,verbose=F)
xgbPred <- matrix(predict(bst, xgbTest), ncol = numClasses, byrow = TRUE)
head(xgbPred)
```

```{r}
gb_target_IndMat<-dummy.data.frame(data=as.data.frame(yTest), 
                                     sep="_", verbose=F, 
                                     dummy.class="ALL")
print(list(RF = rfLogLoss ,XGB = MultiLogLoss(gb_target_IndMat,xgbPred)))
```
```{r}
MultiLogLoss <- function(act, pred)
{
  eps = 1e-15;
  if (!is.matrix(pred)) pred<-t(as.matrix(pred))
  if (!is.matrix(act)) act<-t(as.matrix(act))
  nr <- nrow(pred)
  pred = matrix(sapply( pred, function(x) max(eps,x)), nrow = nr)      
  pred = matrix(sapply( pred, function(x) min(1-eps,x)), nrow = nr)
  #normalize rows
  ll = sum(act*log(sweep(pred, 1, rowSums(pred), FUN="/")))
  ll = -ll/nrow(act)
  return(ll);
}
```

```{r}
param <- list("objective" = "multi:softprob",
              "eval_metric" = "mlogloss",
              "num_class" = numClasses)
cv.nround <- 20
# run xgb cross validation with specified nrounds
bst.cv = xgb.cv(param=param, data = xgbTrain, label = yTrain, 
              nrounds=cv.nround,nfold=cv.nfold,verbose=F)
# run xgboost with same # of rounds
bst = xgboost(param=param, data = xgbTrain, label = yTrain, 
              nrounds=cv.nround,verbose=F)
# predict using bst on xgbTest data
xgbPred <- matrix(predict(bst, xgbTest), ncol = numClasses, byrow = TRUE)

print(list(RF = rfLogLoss ,XGB = MultiLogLoss(gb_target_IndMat,xgbPred)))
```

