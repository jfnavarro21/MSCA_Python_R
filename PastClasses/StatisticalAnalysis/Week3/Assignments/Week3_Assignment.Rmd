---
title: "Week3_Assignment"
author: "John Navarro"
date: "October 14, 2016"
output: pdf_document
---
```{r}
##Set Slope and Intercept and nSample
a<-.8; b<-.1
nSample<-1000
```
#1. Model 1
```{r}


##simulate Eps by setting seed and using rnorm()
set.seed(1112131415)
Eps <- rnorm(nSample, mean = 0, sd = 1.5)

##Simulate X by setting seed and using rnorm()
set.seed(111)
X1 <- rnorm(nSample, mean = 3, sd = 2.5)
## Create Y1 as a linear function
Y1 <- a*X1 + b + Eps
##use all X1 and Y1 to create a dataframe called LinearModel1
LinearModel1 <- as.data.frame(cbind(Y = Y1, X = X1, Eps = Eps))
##Take standard deviations of X1 sample and Eps sample
sd.X <- sd(X1)
sd.Eps <- sd(Eps)
```

```{r}

head(LinearModel1)
##Plot X vs Y
plot(LinearModel1$X,LinearModel1$Y)
##Plot the residuals of the model
plot(LinearModel1$Eps,type="l")
```
#2. Model 2
```{r}
#simulate Eps
set.seed(1112131415)
Eps <- runif(nSample, min = -4.33, max = 4.33)
##Use the same realization of X as Model 1
Y1 <- a*X1 + b + Eps
LinearModel2 <- as.data.frame(cbind(Y = Y1, X = X1, Eps = Eps))
head(LinearModel2)
## Plot X vs Y and plot Eps
plot(LinearModel2$X,LinearModel2$Y)
plot(LinearModel2$Eps,type="l")
```
#3. Model 3
```{r}
#simulate Eps
set.seed(1112131415)
Eps <- rcauchy(nSample, location = 0, scale = 0.3)
##Use the same realization of X as Model 1
Y1 <- a*X1 + b + Eps
LinearModel3 <- as.data.frame(cbind(Y = Y1, X = X1, Eps = Eps))
head(LinearModel3)
## Plot linearmodel 3 and residuals
plot(LinearModel3$X,LinearModel3$Y)
plot(LinearModel3$Eps,type="l")
##Calculate the standard deviation of the residuals
sd(LinearModel3$Eps)
##Generate 5 more Eps samples and their standard deviations
Eps1<-rcauchy(n=nSample,location=0,scale=.3)
Eps2<-rcauchy(n=nSample,location=0,scale=.3)
Eps3<-rcauchy(n=nSample,location=0,scale=.3)
Eps4<-rcauchy(n=nSample,location=0,scale=.3)
Eps5<-rcauchy(n=nSample,location=0,scale=.3)
c(sd(Eps1),sd(Eps2),sd(Eps3),sd(Eps4),sd(Eps5))
```
*How do you interpret this observation?*
Cauchy distributions are very random and extreme, it is not suprising that different samples will give vastly differnt standard deviations.
#4. Model 4
Create the process of standard deviations in which the first 50 observations have sigma=2, followed by 75 observations with sigma=3.4, followed by 75 observations with sigma=0.8 and concluded by 50 observations with sigma=2.6.
```{r}
sd.Values<-c(2,3.4,.8,2.6)
sd.process<-rep(c(rep(sd.Values[1],50),
                  rep(sd.Values[2],75),
                  rep(sd.Values[3],75),
                  rep(sd.Values[4],50)),
            4)
            
plot(sd.process,type="l")
##Create linear model residuals  with the changing SDs and plot
set.seed(1112131415);
Eps<-rnorm(nSample)*sd.process
plot(Eps,type="l")

```
Observe how heteroscedasticity transforms normal distribution into leptokurtic distribution
```{r}
Xvariable<-(100*floor(min(Eps))):(100*ceiling(max(Eps)))
Xvariable<-Xvariable/100
# Plot the sample distribution and the theo. distribution
plot(Xvariable,dnorm(Xvariable,mean=mean(Eps),sd=sd(Eps)),type="l",
      ylim=c(0,.3),col="black",ylab="Distribution of Eps",xlab="")
lines(density(Eps),col="red")
```

```{r}
##Generate LinearModel4 and plot
Y1<-a*X1+b+Eps
LinearModel4<-as.data.frame(cbind(Y=Y1,X=X1))
plot(LinearModel4$X,LinearModel4$Y)
```
#5. Effect of Residual Distribution on Correlation
```{r}
##calculate theoretical 
Theoretical.Rho.Squared<-(a*sd.X)^2/((a*sd.X)^2+sd.Eps^2)
Theoretical.Rho.Squared
##compare with the correlations from the 4 models
c(cor(LinearModel1$X,LinearModel1$Y)^2,
  cor(LinearModel2$X,LinearModel2$Y)^2,
  cor(LinearModel3$X,LinearModel3$Y)^2,
  cor(LinearModel4$X,LinearModel4$Y)^2)

```
*How do you interpret the results?*

The correlation for Linear Model 1 is very close to the value for Theoretical.Rho.Squared. While the other 3 models produce correlations that are different and lower than the theoretical value. The correlation from the Cauchy model is vastly different and extremely low. While the correlations for the uniform and heteroscedastic models are lower than normal model correlation, they are still somewhat close. This tells me that these two models explain some of the correlation that is seen, but not all of it.

#6. Estimation of Linear Model
```{r}
##Create a linear regression using X&Y data from LinearModel1
m1<-lm(Y~X,data=LinearModel1)
summary(m1)
names(summary(m1))
##look at the parameters
summary(m1)$r.squared
summary(m1)$coeff
summary(m1)$sigma^2
var(summary(m1)$residuals)
##reconcile sigmaEstimate with m1$sigma by normalizing with respect to deg of freedom
var(summary(m1)$residuals)*999/998
```
Estimate the same parameters using the method of moments directly.
```{r}
##Solve for aEstimate, then bEstimate, then sigmaEstimate using LinearModel1 data
aEstimate <- cov(LinearModel1$X,LinearModel1$Y)/var(LinearModel1$X)
bEstimate <- mean(LinearModel1$Y) - (aEstimate * mean(LinearModel1$X))
sigmaEstimate <- sqrt(var(LinearModel1$Y) - ((aEstimate^2) * (var(LinearModel1$X))))
```
The result of estimation by method of moments is:
```{r}
c(aEstimate,bEstimate,sigmaEstimate)
```
Reconcile sigmaEstimate with m1$sigma.
```{r}
##normalize m1$sigma^2 with respect to their degrees of freedom
c(sigmaMetodMoments=sigmaEstimate,sigmaLinearModel=summary(m1)$sigma)
reconciled.sigmaLinearModel <- sqrt(((summary(m1)$sigma)^2) * (998/999))
c(sigmaMethodMoments = sigmaEstimate,reconciledSigmaLinModel = reconciled.sigmaLinearModel)
```
#7. Fit lm() to the the Rest of Linear Models
Compare the differences between the assumptions of the 4 models and tell how they change the model behavior and estimated parameters.
```{r}
##fit the other models into linear regressions and look at parameters
m2<-lm(Y~X,data=LinearModel2)
summary(m2)
names(summary(m2))
summary(m2)$coeff
summary(m2)$sigma
summary(m2)$r.squared
summary(m2)$df

m3<-lm(Y~X,data=LinearModel3)
summary(m3)
names(summary(m3))
summary(m3)$coeff
summary(m3)$sigma
summary(m3)$r.squared
summary(m3)$df

m4<-lm(Y~X,data=LinearModel4)
summary(m4)
names(summary(m4))
summary(m4)$coeff
summary(m4)$sigma
summary(m4)$r.squared
summary(m4)$df
```
#Compare the differences between the assumptions of the 4 models and tell how they change the model behavior and estimated parameters.

The four linear models all use different distribution patterns to simulate the values of Eps in the equation Y=aX + B + Eps. The model 1 uses a normal distribution which gives us the closest match to the a, B, and sigma values attained from the data using Method of Moments. This tells us that the distribution of error terms in the data is normal.

The second linear model uses a uniform distribution to simulate the values of Eps between +/-4.33. This gives us slightly different parameters. The slope is slightly steeper, with a lower intercept, but a higher sigma. A uniform distribution should have a higher standard deviation for its error terms than a normal distribution, which would have them clustered around the mean, with small tails.

The third linear model uses a cauchy distribution to simulate the values of Eps. As we can see in the above plots, the model generates extreme outliers. This gives us a smaller slope, larger intercept, but a much, much higher sigma. Which tells us just how large the variance of the error terms are that were created with this model.

The fourth linear model uses a heteroscedastic distribution for its Eps values. This means that the standard deviation parameters vary during the generation of values by the normal distribution. Given this, we see that the Eps values have a larger standard deviation than a normal distribution. As well as a steeper slope and a lower intercept than the normal linear model. 

